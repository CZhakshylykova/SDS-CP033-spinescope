{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5404d83",
   "metadata": {},
   "source": [
    "# üè• Comprehensive Exploratory Data Analysis\n",
    "## Biomechanical Features of Orthopedic Patients\n",
    "\n",
    "### üìã Executive Summary\n",
    "This notebook provides a comprehensive exploratory data analysis of biomechanical features in orthopedic patients. The analysis includes statistical tests, advanced visualizations, dimensionality reduction, and clustering techniques to understand the relationship between biomechanical measurements and orthopedic conditions.\n",
    "\n",
    "### üéØ Key Objectives\n",
    "1. **Dataset Overview**: Understand data structure, missing values, and basic statistics\n",
    "2. **Target Analysis**: Examine class distribution and balance\n",
    "3. **Feature Analysis**: Analyze numerical features, distributions, and outliers\n",
    "4. **Correlation Analysis**: Identify multicollinearity and feature relationships\n",
    "5. **Statistical Testing**: Perform hypothesis testing for group differences\n",
    "6. **Dimensionality Reduction**: Apply PCA and t-SNE for visualization\n",
    "7. **Clustering**: Discover natural groupings in the data\n",
    "\n",
    "### üìä Dataset Information\n",
    "- **Source**: UCI Machine Learning Repository\n",
    "- **Features**: 6 biomechanical attributes\n",
    "- **Target**: 3-class (Normal, Hernia, Spondylolisthesis) and binary classification\n",
    "- **Size**: 310 patient records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f702c",
   "metadata": {},
   "source": [
    "### Comprehensive Exploratory Data Analysis\n",
    "##### Biomechanical Features of Orthopedic Patients\n",
    "This notebook provides a comprehensive exploratory data analysis of biomechanical features in orthopedic patients, including statistical tests, visualizations, and dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81880f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration parameters loaded successfully!\n",
      "Available parameters:\n",
      "  random_state: 42\n",
      "  test_size: 0.2\n",
      "  significance_level: 0.05\n",
      "  correlation_threshold: 0.8\n",
      "  pca_n_components: 2\n",
      "  tsne_n_components: 2\n",
      "  tsne_perplexity: 30\n",
      "  umap_n_components: 2\n",
      "  umap_n_neighbors: 15\n",
      "  figure_size: (12, 8)\n",
      "  plot_dpi: 300\n",
      "  save_plots: True\n",
      "  plots_folder: plots\n"
     ]
    }
   ],
   "source": [
    "# Configuration Parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'random_state': 42,\n",
    "    'test_size': 0.2,\n",
    "    'significance_level': 0.05,\n",
    "    'correlation_threshold': 0.8,\n",
    "    'pca_n_components': 2,\n",
    "    'tsne_n_components': 2,\n",
    "    'tsne_perplexity': 30,\n",
    "    'umap_n_components': 2,\n",
    "    'umap_n_neighbors': 15,\n",
    "    'figure_size': (12, 8),\n",
    "    'plot_dpi': 300,\n",
    "    'save_plots': True,\n",
    "    'plots_folder': 'plots'\n",
    "}\n",
    "\n",
    "print(\"Configuration parameters loaded successfully!\")\n",
    "print(\"Available parameters:\")\n",
    "for key, value in ANALYSIS_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0f5682a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiOut class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Custom output handler for better visualization\n",
    "class MultiOut:\n",
    "    def __init__(self, *outputs):\n",
    "        self.outputs = outputs\n",
    "    \n",
    "    def write(self, s):\n",
    "        for out in self.outputs:\n",
    "            out.write(s)\n",
    "    \n",
    "    def flush(self):\n",
    "        for out in self.outputs:\n",
    "            out.flush()\n",
    "\n",
    "print(\"MultiOut class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd6e0a",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Analysis Configuration\n",
    "You can modify these parameters to customize the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baa7b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Analysis Configuration:\n",
      "  correlation_threshold: 0.7\n",
      "  outlier_z_threshold: 3.0\n",
      "  significance_level: 0.05\n",
      "  pca_variance_threshold: 0.95\n",
      "  figure_size: (15, 10)\n",
      "  save_plots: True\n",
      "  plot_format: png\n",
      "  random_state: 42\n"
     ]
    }
   ],
   "source": [
    "# Analysis Configuration Parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'correlation_threshold': 0.7,  # Threshold for high correlation\n",
    "    'outlier_z_threshold': 3.0,    # Z-score threshold for outliers\n",
    "    'significance_level': 0.05,    # P-value threshold for statistical tests\n",
    "    'pca_variance_threshold': 0.95, # Variance threshold for PCA\n",
    "    'figure_size': (15, 10),       # Default figure size\n",
    "    'save_plots': True,            # Whether to save plots\n",
    "    'plot_format': 'png',          # Plot save format\n",
    "    'random_state': 42             # Random state for reproducibility\n",
    "}\n",
    "\n",
    "print(\"üìã Analysis Configuration:\")\n",
    "for key, value in ANALYSIS_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fba234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n",
      "Available packages:\n",
      "- pandas, numpy, matplotlib, seaborn\n",
      "- scipy, sklearn, umap\n",
      "- kagglehub for data access\n",
      "- warnings and os for system operations\n",
      "üìä Matplotlib configured for inline display + file saving\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import levene\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import kagglehub\n",
    "import umap\n",
    "\n",
    "# Set up plotting configuration for inline display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Available packages:\")\n",
    "print(\"- pandas, numpy, matplotlib, seaborn\")\n",
    "print(\"- scipy, sklearn, umap\")\n",
    "print(\"- kagglehub for data access\")\n",
    "print(\"- warnings and os for system operations\")\n",
    "print(\"üìä Matplotlib configured for inline display + file saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c511d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created plots directory: plots\n",
      "üöÄ Loading dataset...\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/sammy123/lower-back-pain-symptoms-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 19.8k/19.8k [00:00<00:00, 155kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Dataset downloaded to: /Users/cholponzhakshylykova/.cache/kagglehub/datasets/sammy123/lower-back-pain-symptoms-dataset/versions/1\n",
      "Dataset loaded successfully! Shape: (310, 14)\n",
      "‚úÖ Dataset loaded successfully!\n",
      "üìä Dataset shape: (310, 14)\n",
      "üìã Columns: ['Col1', 'Col2', 'Col3', 'Col4', 'Col5', 'Col6', 'Col7', 'Col8', 'Col9', 'Col10', 'Col11', 'Col12', 'Class_att', 'Unnamed: 13']\n",
      "\n",
      "üîß Data loading complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create plots directory and load data\n",
    "plots_folder = ANALYSIS_CONFIG['plots_folder']\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)\n",
    "    print(f\"Created plots directory: {plots_folder}\")\n",
    "else:\n",
    "    print(f\"Plots directory already exists: {plots_folder}\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Vertebral Column dataset\"\"\"\n",
    "    try:\n",
    "        # Download latest version\n",
    "        path = kagglehub.dataset_download(\"sammy123/lower-back-pain-symptoms-dataset\")\n",
    "        print(f\"Dataset downloaded to: {path}\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        data_path = os.path.join(path, \"Dataset_spine.csv\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the data immediately\n",
    "print(\"üöÄ Loading dataset...\")\n",
    "df = load_data()\n",
    "\n",
    "if df is not None:\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset. Please check the data source.\")\n",
    "    \n",
    "print(\"\\nüîß Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ead59",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bddbaeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. DATASET OVERVIEW\n",
      "==================================================\n",
      "Dataset Shape: (310, 14)\n",
      "Features: 14\n",
      "Samples: 310\n",
      "\n",
      "Column Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 310 entries, 0 to 309\n",
      "Data columns (total 14 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Col1         310 non-null    float64\n",
      " 1   Col2         310 non-null    float64\n",
      " 2   Col3         310 non-null    float64\n",
      " 3   Col4         310 non-null    float64\n",
      " 4   Col5         310 non-null    float64\n",
      " 5   Col6         310 non-null    float64\n",
      " 6   Col7         310 non-null    float64\n",
      " 7   Col8         310 non-null    float64\n",
      " 8   Col9         310 non-null    float64\n",
      " 9   Col10        310 non-null    float64\n",
      " 10  Col11        310 non-null    float64\n",
      " 11  Col12        310 non-null    float64\n",
      " 12  Class_att    310 non-null    object \n",
      " 13  Unnamed: 13  14 non-null     object \n",
      "dtypes: float64(12), object(2)\n",
      "memory usage: 34.0+ KB\n",
      "None\n",
      "\n",
      "Data Types:\n",
      "float64    12\n",
      "object      2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing Values:\n",
      "Unnamed: 13    296\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows:\n",
      "        Col1       Col2       Col3       Col4        Col5       Col6  \\\n",
      "0  63.027817  22.552586  39.609117  40.475232   98.672917  -0.254400   \n",
      "1  39.056951  10.060991  25.015378  28.995960  114.405425   4.564259   \n",
      "2  68.832021  22.218482  50.092194  46.613539  105.985135  -3.530317   \n",
      "3  69.297008  24.652878  44.311238  44.644130  101.868495  11.211523   \n",
      "4  49.712859   9.652075  28.317406  40.060784  108.168725   7.918501   \n",
      "\n",
      "       Col7     Col8     Col9     Col10      Col11    Col12 Class_att  \\\n",
      "0  0.744503  12.5661  14.5386  15.30468 -28.658501  43.5123  Abnormal   \n",
      "1  0.415186  12.8874  17.5323  16.78486 -25.530607  16.1102  Abnormal   \n",
      "2  0.474889  26.8343  17.4861  16.65897 -29.031888  19.2221  Abnormal   \n",
      "3  0.369345  23.5603  12.7074  11.42447 -30.470246  18.8329  Abnormal   \n",
      "4  0.543360  35.4940  15.9546   8.87237 -16.378376  24.9171  Abnormal   \n",
      "\n",
      "                                         Unnamed: 13  \n",
      "0                                                NaN  \n",
      "1                                                NaN  \n",
      "2  Prediction is done by using binary classificat...  \n",
      "3                                                NaN  \n",
      "4                                                NaN  \n",
      "\n",
      "Last 5 rows:\n",
      "          Col1       Col2       Col3       Col4        Col5      Col6  \\\n",
      "305  47.903565  13.616688  36.000000  34.286877  117.449062 -4.245395   \n",
      "306  53.936748  20.721496  29.220534  33.215251  114.365845 -0.421010   \n",
      "307  61.446597  22.694968  46.170347  38.751628  125.670725 -2.707880   \n",
      "308  45.252792   8.693157  41.583126  36.559635  118.545842  0.214750   \n",
      "309  33.841641   5.073991  36.641233  28.767649  123.945244 -0.199249   \n",
      "\n",
      "         Col7     Col8     Col9     Col10      Col11    Col12 Class_att  \\\n",
      "305  0.129744   7.8433  14.7484   8.51707 -15.728927  11.5472    Normal   \n",
      "306  0.047913  19.1986  18.1972   7.08745   6.013843  43.8693    Normal   \n",
      "307  0.081070  16.2059  13.5565   8.89572   3.564463  18.4151    Normal   \n",
      "308  0.159251  14.7334  16.0928   9.75922   5.767308  33.7192    Normal   \n",
      "309  0.674504  19.3825  17.6963  13.72929   1.783007  40.6049    Normal   \n",
      "\n",
      "    Unnamed: 13  \n",
      "305         NaN  \n",
      "306         NaN  \n",
      "307         NaN  \n",
      "308         NaN  \n",
      "309         NaN  \n",
      "\n",
      "Basic Statistics:\n",
      "             Col1        Col2        Col3        Col4        Col5        Col6  \\\n",
      "count  310.000000  310.000000  310.000000  310.000000  310.000000  310.000000   \n",
      "mean    60.496653   17.542822   51.930930   42.953831  117.920655   26.296694   \n",
      "std     17.236520   10.008330   18.554064   13.423102   13.317377   37.559027   \n",
      "min     26.147921   -6.554948   14.000000   13.366931   70.082575  -11.058179   \n",
      "25%     46.430294   10.667069   37.000000   33.347122  110.709196    1.603727   \n",
      "50%     58.691038   16.357689   49.562398   42.404912  118.268178   11.767934   \n",
      "75%     72.877696   22.120395   63.000000   52.695888  125.467674   41.287352   \n",
      "max    129.834041   49.431864  125.742385  121.429566  163.071041  418.543082   \n",
      "\n",
      "             Col7        Col8        Col9       Col10       Col11       Col12  \n",
      "count  310.000000  310.000000  310.000000  310.000000  310.000000  310.000000  \n",
      "mean     0.472979   21.321526   13.064511   11.933317  -14.053139   25.645981  \n",
      "std      0.285787    8.639423    3.399713    2.893265   12.225582   10.450558  \n",
      "min      0.003220    7.027000    7.037800    7.030600  -35.287375    7.007900  \n",
      "25%      0.224367   13.054400   10.417800    9.541140  -24.289522   17.189075  \n",
      "50%      0.475989   21.907150   12.938450   11.953835  -14.622856   24.931950  \n",
      "75%      0.704846   28.954075   15.889525   14.371810   -3.497094   33.979600  \n",
      "max      0.998827   36.743900   19.324000   16.821080    6.972071   44.341200  \n",
      "\n",
      "Duplicate rows: 0\n",
      "\n",
      "Memory usage: 0.06 MB\n"
     ]
    }
   ],
   "source": [
    "def dataset_overview(df):\n",
    "    \"\"\"\n",
    "    Provide comprehensive overview of the dataset including:\n",
    "    - Basic information (shape, columns, data types)\n",
    "    - Missing values analysis\n",
    "    - Basic statistics\n",
    "    - Data quality assessment\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. DATASET OVERVIEW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Features: {df.shape[1]}\")\n",
    "    print(f\"Samples: {df.shape[0]}\")\n",
    "    \n",
    "    print(\"\\nColumn Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    dataset_overview(df)\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852700f",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb983e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. TARGET VARIABLE ANALYSIS\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SDS/SDS-CP033-spinescope/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'class'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 62\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Execute immediately\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[43mtarget_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclass\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå No data available for analysis\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtarget_analysis\u001b[39m\u001b[34m(df, target_col)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m50\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Value counts and proportions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m value_counts = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[43m]\u001b[49m.value_counts()\n\u001b[32m     14\u001b[39m proportions = df[target_col].value_counts(normalize=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTarget variable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_col\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SDS/SDS-CP033-spinescope/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/SDS/SDS-CP033-spinescope/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'class'"
     ]
    }
   ],
   "source": [
    "def target_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze the target variable including:\n",
    "    - Value counts and proportions\n",
    "    - Visualization of class distribution\n",
    "    - Class balance assessment\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"2. TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Value counts and proportions\n",
    "    value_counts = df[target_col].value_counts()\n",
    "    proportions = df[target_col].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"Target variable: {target_col}\")\n",
    "    print(f\"Unique classes: {df[target_col].nunique()}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    \n",
    "    for class_name, count in value_counts.items():\n",
    "        percentage = proportions[class_name] * 100\n",
    "        print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Count plot\n",
    "    sns.countplot(data=df, x=target_col, ax=axes[0])\n",
    "    axes[0].set_title('Class Distribution (Count)')\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[0].text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1].set_title('Class Distribution (Proportion)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/target_distribution_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Class balance assessment\n",
    "    max_class = value_counts.max()\n",
    "    min_class = value_counts.min()\n",
    "    balance_ratio = min_class / max_class\n",
    "    \n",
    "    print(f\"\\nClass Balance Assessment:\")\n",
    "    print(f\"Most frequent class: {value_counts.idxmax()} ({max_class} samples)\")\n",
    "    print(f\"Least frequent class: {value_counts.idxmin()} ({min_class} samples)\")\n",
    "    print(f\"Balance ratio: {balance_ratio:.3f}\")\n",
    "    \n",
    "    if balance_ratio < 0.5:\n",
    "        print(\"‚ö†Ô∏è  Dataset is imbalanced - consider using stratified sampling or class weighting\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset is relatively balanced\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    target_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45930",
   "metadata": {},
   "source": [
    "## 4. Numerical Features Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_features_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze numerical features including:\n",
    "    - Distribution plots\n",
    "    - Box plots by class\n",
    "    - Statistical summaries by class\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"3. NUMERICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "    print(f\"Features: {numerical_cols}\")\n",
    "    \n",
    "    # Distribution plots\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        axes[row, col_idx].hist(df[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/numerical_features_distribution_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plots by class\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        sns.boxplot(data=df, x=target_col, y=col, ax=axes[row, col_idx])\n",
    "        axes[row, col_idx].set_title(f'{col} by {target_col}')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/numerical_features_boxplots_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summaries by class\n",
    "    print(\"\\nStatistical Summary by Class:\")\n",
    "    for class_name in df[target_col].unique():\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(df[df[target_col] == class_name][numerical_cols].describe())\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    numerical_features_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7f4b3",
   "metadata": {},
   "source": [
    "## 5. Outlier Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b762fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Detect and analyze outliers using:\n",
    "    - IQR method\n",
    "    - Z-score method\n",
    "    - Isolation Forest\n",
    "    - Visualization of outliers\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"4. OUTLIER DETECTION AND ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\nAnalyzing outliers in {col}:\")\n",
    "        \n",
    "        # IQR method\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        iqr_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        \n",
    "        # Z-score method\n",
    "        z_scores = np.abs(stats.zscore(df[col]))\n",
    "        z_outliers = df[z_scores > 3]\n",
    "        \n",
    "        print(f\"  IQR method: {len(iqr_outliers)} outliers ({len(iqr_outliers)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Z-score method: {len(z_outliers)} outliers ({len(z_outliers)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  IQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        \n",
    "        outlier_summary[col] = {\n",
    "            'IQR_outliers': len(iqr_outliers),\n",
    "            'Z_outliers': len(z_outliers),\n",
    "            'IQR_percentage': len(iqr_outliers)/len(df)*100,\n",
    "            'Z_percentage': len(z_outliers)/len(df)*100\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Box plot for all features\n",
    "    df_scaled = df[numerical_cols].copy()\n",
    "    for col in numerical_cols:\n",
    "        df_scaled[col] = (df_scaled[col] - df_scaled[col].mean()) / df_scaled[col].std()\n",
    "    \n",
    "    axes[0, 0].boxplot([df_scaled[col] for col in numerical_cols])\n",
    "    axes[0, 0].set_xticklabels(numerical_cols, rotation=45)\n",
    "    axes[0, 0].set_title('Box Plot (Standardized Features)')\n",
    "    axes[0, 0].set_ylabel('Standardized Values')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Outlier count by method\n",
    "    methods = ['IQR', 'Z-score']\n",
    "    iqr_counts = [outlier_summary[col]['IQR_outliers'] for col in numerical_cols]\n",
    "    z_counts = [outlier_summary[col]['Z_outliers'] for col in numerical_cols]\n",
    "    \n",
    "    x = np.arange(len(numerical_cols))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, iqr_counts, width, label='IQR', alpha=0.7)\n",
    "    axes[0, 1].bar(x + width/2, z_counts, width, label='Z-score', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Features')\n",
    "    axes[0, 1].set_ylabel('Number of Outliers')\n",
    "    axes[0, 1].set_title('Outlier Count by Method')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(numerical_cols, rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Outlier percentage\n",
    "    iqr_percentages = [outlier_summary[col]['IQR_percentage'] for col in numerical_cols]\n",
    "    z_percentages = [outlier_summary[col]['Z_percentage'] for col in numerical_cols]\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, iqr_percentages, width, label='IQR', alpha=0.7)\n",
    "    axes[1, 0].bar(x + width/2, z_percentages, width, label='Z-score', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Features')\n",
    "    axes[1, 0].set_ylabel('Percentage of Outliers')\n",
    "    axes[1, 0].set_title('Outlier Percentage by Method')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(numerical_cols, rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary heatmap\n",
    "    summary_df = pd.DataFrame(outlier_summary).T\n",
    "    sns.heatmap(summary_df[['IQR_percentage', 'Z_percentage']], \n",
    "                annot=True, fmt='.1f', cmap='Reds', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Outlier Percentage Heatmap')\n",
    "    axes[1, 1].set_ylabel('Features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/outlier_detection_summary.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Overall summary\n",
    "    print(\"\\nOutlier Summary:\")\n",
    "    summary_df = pd.DataFrame(outlier_summary).T\n",
    "    print(summary_df.round(2))\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    outlier_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4ed50",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15023eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze correlations between features:\n",
    "    - Correlation matrix\n",
    "    - Heatmap visualization\n",
    "    - Identify highly correlated features\n",
    "    - Multicollinearity assessment\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"5. CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    print(f\"Correlation matrix shape: {corr_matrix.shape}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Standard correlation heatmap\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='RdBu_r', center=0, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Correlation Matrix (Lower Triangle)')\n",
    "    \n",
    "    # Clustermap for grouping similar correlations\n",
    "    corr_linkage = sns.clustermap(corr_matrix, method='ward', cmap='RdBu_r', \n",
    "                                 center=0, figsize=(10, 8), \n",
    "                                 cbar_pos=(0.02, 0.8, 0.05, 0.15))\n",
    "    axes[0, 1].set_title('Correlation Clustermap')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # High correlation pairs\n",
    "    threshold = ANALYSIS_CONFIG['correlation_threshold']\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature1': corr_matrix.columns[i],\n",
    "                    'Feature2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nHighly correlated feature pairs (|r| > {threshold}):\")\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f\"  {pair['Feature1']} - {pair['Feature2']}: {pair['Correlation']:.3f}\")\n",
    "        \n",
    "        # Visualize high correlation pairs\n",
    "        high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "        high_corr_df['abs_correlation'] = high_corr_df['Correlation'].abs()\n",
    "        \n",
    "        bars = axes[1, 0].bar(range(len(high_corr_df)), high_corr_df['abs_correlation'])\n",
    "        axes[1, 0].set_xlabel('Feature Pairs')\n",
    "        axes[1, 0].set_ylabel('|Correlation|')\n",
    "        axes[1, 0].set_title(f'High Correlation Pairs (|r| > {threshold})')\n",
    "        axes[1, 0].set_xticks(range(len(high_corr_df)))\n",
    "        axes[1, 0].set_xticklabels([f\"{pair['Feature1']}-{pair['Feature2']}\" for pair in high_corr_pairs], \n",
    "                                  rotation=45, ha='right')\n",
    "        axes[1, 0].axhline(y=threshold, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        print(f\"\\nNo highly correlated feature pairs found (|r| > {threshold})\")\n",
    "        axes[1, 0].text(0.5, 0.5, f'No high correlations\\n(|r| > {threshold})', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('High Correlation Analysis')\n",
    "    \n",
    "    # Multicollinearity assessment using VIF\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    try:\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = numerical_cols\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(df[numerical_cols].values, i) \n",
    "                          for i in range(len(numerical_cols))]\n",
    "        \n",
    "        # Plot VIF values\n",
    "        bars = axes[1, 1].bar(range(len(vif_data)), vif_data['VIF'])\n",
    "        axes[1, 1].set_xlabel('Features')\n",
    "        axes[1, 1].set_ylabel('VIF Value')\n",
    "        axes[1, 1].set_title('Variance Inflation Factor (VIF)')\n",
    "        axes[1, 1].set_xticks(range(len(vif_data)))\n",
    "        axes[1, 1].set_xticklabels(vif_data['Feature'], rotation=45, ha='right')\n",
    "        axes[1, 1].axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='Moderate (5)')\n",
    "        axes[1, 1].axhline(y=10, color='red', linestyle='--', alpha=0.7, label='High (10)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        print(\"\\nVariance Inflation Factor (VIF) Analysis:\")\n",
    "        print(vif_data.round(2))\n",
    "        \n",
    "        high_vif = vif_data[vif_data['VIF'] > 5]\n",
    "        if not high_vif.empty:\n",
    "            print(f\"\\nFeatures with high VIF (>5):\")\n",
    "            print(high_vif)\n",
    "        else:\n",
    "            print(\"\\nNo features with high multicollinearity detected (VIF > 5)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating VIF: {e}\")\n",
    "        axes[1, 1].text(0.5, 0.5, 'VIF calculation\\nfailed', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/correlation_analysis.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save clustermap separately\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.clustermap(corr_matrix, method='ward', cmap='RdBu_r', center=0, \n",
    "                  cbar_pos=(0.02, 0.8, 0.05, 0.15))\n",
    "    plt.savefig(f\"{plots_folder}/correlation_clustermap.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    correlation_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c59664",
   "metadata": {},
   "source": [
    "## 7. Feature Relationships Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daf4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_relationships(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze relationships between features:\n",
    "    - Pairplot\n",
    "    - Feature interactions\n",
    "    - Violin plots by class\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"6. FEATURE RELATIONSHIPS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Pairplot (sample if too many features)\n",
    "    if len(numerical_cols) > 6:\n",
    "        print(f\"Too many features ({len(numerical_cols)}) for pairplot. Showing first 6.\")\n",
    "        cols_for_pairplot = numerical_cols[:6]\n",
    "    else:\n",
    "        cols_for_pairplot = numerical_cols\n",
    "    \n",
    "    print(f\"Creating pairplot for features: {cols_for_pairplot}\")\n",
    "    \n",
    "    # Create pairplot\n",
    "    pairplot_df = df[cols_for_pairplot + [target_col]]\n",
    "    g = sns.pairplot(pairplot_df, hue=target_col, diag_kind='kde', \n",
    "                     plot_kws={'alpha': 0.6}, diag_kws={'alpha': 0.7})\n",
    "    g.fig.suptitle('Feature Pairplot by Class', y=1.02)\n",
    "    \n",
    "    plt.savefig(f\"{plots_folder}/feature_pairplot.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature interactions - violin plots\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        sns.violinplot(data=df, x=target_col, y=col, ax=axes[row, col_idx])\n",
    "        axes[row, col_idx].set_title(f'{col} Distribution by {target_col}')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/feature_interactions_violin_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance using correlation with target\n",
    "    if target_col in df.columns:\n",
    "        # Create numerical encoding of target for correlation\n",
    "        target_encoded = pd.get_dummies(df[target_col], drop_first=True)\n",
    "        \n",
    "        if target_encoded.shape[1] == 1:  # Binary classification\n",
    "            target_numeric = target_encoded.iloc[:, 0]\n",
    "            correlations = df[numerical_cols].corrwith(target_numeric)\n",
    "            \n",
    "            print(\"\\nFeature-Target Correlations:\")\n",
    "            correlations_sorted = correlations.abs().sort_values(ascending=False)\n",
    "            print(correlations_sorted.round(3))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            correlations_sorted.plot(kind='bar')\n",
    "            plt.title('Feature Importance (Absolute Correlation with Target)')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('|Correlation|')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plots_folder}/feature_importance_correlation.png\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Multi-class target detected - correlation analysis skipped\")\n",
    "    \n",
    "    print(f\"\\nFeature relationship analysis completed for {len(numerical_cols)} features\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    feature_relationships(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb6f84",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eba347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction analysis:\n",
    "    - PCA analysis with explained variance\n",
    "    - t-SNE visualization\n",
    "    - UMAP visualization\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"7. DIMENSIONALITY REDUCTION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numerical_cols].dropna()\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA Analysis\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"Original dimensions: {X_scaled.shape}\")\n",
    "    print(f\"PCA components: {pca.n_components_}\")\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    print(f\"First 2 components explain {cumulative_variance[1]:.3f} of variance\")\n",
    "    print(f\"First 3 components explain {cumulative_variance[2]:.3f} of variance\")\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "    print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "    \n",
    "    # t-SNE\n",
    "    print(\"\\nComputing t-SNE...\")\n",
    "    tsne = TSNE(n_components=ANALYSIS_CONFIG['tsne_n_components'], \n",
    "                perplexity=ANALYSIS_CONFIG['tsne_perplexity'],\n",
    "                random_state=ANALYSIS_CONFIG['random_state'])\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP\n",
    "    print(\"Computing UMAP...\")\n",
    "    reducer = umap.UMAP(n_components=ANALYSIS_CONFIG['umap_n_components'],\n",
    "                       n_neighbors=ANALYSIS_CONFIG['umap_n_neighbors'],\n",
    "                       random_state=ANALYSIS_CONFIG['random_state'])\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # PCA explained variance\n",
    "    axes[0, 0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "    axes[0, 0].set_xlabel('Principal Component')\n",
    "    axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[0, 0].set_title('PCA Explained Variance')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative explained variance\n",
    "    axes[0, 1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "    axes[0, 1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%')\n",
    "    axes[0, 1].set_xlabel('Number of Components')\n",
    "    axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[0, 1].set_title('Cumulative Explained Variance')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PCA visualization\n",
    "    scatter = axes[0, 2].scatter(X_pca[:, 0], X_pca[:, 1], c=y.astype('category').cat.codes, \n",
    "                                cmap='viridis', alpha=0.6)\n",
    "    axes[0, 2].set_xlabel(f'PC1 ({explained_variance_ratio[0]:.3f})')\n",
    "    axes[0, 2].set_ylabel(f'PC2 ({explained_variance_ratio[1]:.3f})')\n",
    "    axes[0, 2].set_title('PCA Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[0, 2])\n",
    "    \n",
    "    # t-SNE visualization\n",
    "    scatter = axes[1, 0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y.astype('category').cat.codes, \n",
    "                                cmap='viridis', alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('t-SNE 1')\n",
    "    axes[1, 0].set_ylabel('t-SNE 2')\n",
    "    axes[1, 0].set_title('t-SNE Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # UMAP visualization\n",
    "    scatter = axes[1, 1].scatter(X_umap[:, 0], X_umap[:, 1], c=y.astype('category').cat.codes, \n",
    "                                cmap='viridis', alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('UMAP 1')\n",
    "    axes[1, 1].set_ylabel('UMAP 2')\n",
    "    axes[1, 1].set_title('UMAP Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[1, 1])\n",
    "    \n",
    "    # Feature loadings for first two PCs\n",
    "    loadings = pca.components_[:2].T\n",
    "    axes[1, 2].scatter(loadings[:, 0], loadings[:, 1], alpha=0.7)\n",
    "    for i, feature in enumerate(numerical_cols):\n",
    "        axes[1, 2].annotate(feature, (loadings[i, 0], loadings[i, 1]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[1, 2].set_xlabel('PC1 Loading')\n",
    "    axes[1, 2].set_ylabel('PC2 Loading')\n",
    "    axes[1, 2].set_title('Feature Loadings (PC1 vs PC2)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/dimensionality_reduction_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance from PCA\n",
    "    print(\"\\nFeature importance from first 2 PCs:\")\n",
    "    feature_importance = np.abs(loadings).mean(axis=1)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': numerical_cols,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance_df.round(3))\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    dimensionality_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71b3d3",
   "metadata": {},
   "source": [
    "## 9. Clustering Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b095a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Perform clustering analysis:\n",
    "    - K-means clustering with optimal k selection\n",
    "    - Elbow method for k selection\n",
    "    - Silhouette analysis\n",
    "    - Comparison with true labels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"8. CLUSTERING ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numerical_cols].dropna()\n",
    "    \n",
    "    # Standardize features for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Elbow method: calculate inertia for different k values\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=ANALYSIS_CONFIG['random_state'], n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Silhouette analysis for optimal k\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, 11):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=ANALYSIS_CONFIG['random_state'], n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Find optimal k based on silhouette score\n",
    "    optimal_k = range(2, 11)[np.argmax(silhouette_scores)]\n",
    "    print(f\"Optimal number of clusters (silhouette): {optimal_k}\")\n",
    "    print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "    # Perform clustering with optimal k\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=ANALYSIS_CONFIG['random_state'], n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    df_cluster = df.copy()\n",
    "    df_cluster['Cluster'] = cluster_labels\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Elbow method plot\n",
    "    axes[0, 0].plot(k_range, inertias, 'bo-')\n",
    "    axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 0].set_ylabel('Inertia')\n",
    "    axes[0, 0].set_title('Elbow Method for Optimal k')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette score plot\n",
    "    axes[0, 1].plot(range(2, 11), silhouette_scores, 'ro-')\n",
    "    axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].set_title('Silhouette Analysis')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Clusters in PCA space\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "    scatter = axes[1, 0].scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('First Principal Component')\n",
    "    axes[1, 0].set_ylabel('Second Principal Component')\n",
    "    axes[1, 0].set_title('Clusters in PCA Space')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # Clusters vs true labels\n",
    "    cluster_crosstab = pd.crosstab(df_cluster['Cluster'], df_cluster[target_col])\n",
    "    sns.heatmap(cluster_crosstab, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Clusters vs True Labels')\n",
    "    axes[1, 1].set_xlabel('True Labels')\n",
    "    axes[1, 1].set_ylabel('Clusters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/clustering_analysis_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCluster vs True Labels Cross-tabulation:\")\n",
    "    print(cluster_crosstab)\n",
    "\n",
    "\n",
    "def statistical_tests(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Perform statistical tests to identify significant differences between groups\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"9. STATISTICAL TESTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    groups = df.groupby(target_col)\n",
    "    group_names = list(groups.groups.keys())\n",
    "    stats_results = []\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\nFeature: {col}\")\n",
    "        print(\"-\" * 20)\n",
    "        feat_group_data = [df[df[target_col] == name][col].dropna() for name in group_names]\n",
    "        \n",
    "        # Test for normality\n",
    "        normality = []\n",
    "        for i, data in enumerate(feat_group_data):\n",
    "            if len(data) > 3:\n",
    "                _, pval = stats.shapiro(data)\n",
    "                normality.append(pval > 0.05)\n",
    "                print(f\"  {group_names[i]}: Shapiro p={pval:.4f} ({'Normal' if pval>0.05 else 'Non-normal'})\")\n",
    "            else:\n",
    "                normality.append(False)\n",
    "                print(f\"  {group_names[i]}: Not enough data for normality test\")\n",
    "        \n",
    "        # Test for equal variances\n",
    "        if all([len(x)>3 for x in feat_group_data]):\n",
    "            _, p_levene = levene(*feat_group_data)\n",
    "            print(f\"  Levene's p={p_levene:.4f} ({'Equal variances' if p_levene>0.05 else 'Unequal variances'})\")\n",
    "        else:\n",
    "            p_levene = np.nan\n",
    "        \n",
    "        # Choose appropriate test\n",
    "        if all(normality) and (p_levene > 0.05):\n",
    "            # Use ANOVA if data is normal and variances are equal\n",
    "            f_stat, p_anova = stats.f_oneway(*feat_group_data)\n",
    "            print(f\"  ANOVA F={f_stat:.3f}, p={p_anova:.4e}\")\n",
    "            test_type = \"ANOVA\"\n",
    "            test_stat = f_stat\n",
    "            p_val = p_anova\n",
    "        else:\n",
    "            # Use Kruskal-Wallis if assumptions are violated\n",
    "            h_stat, p_kruskal = stats.kruskal(*feat_group_data)\n",
    "            print(f\"  Kruskal-Wallis H={h_stat:.3f}, p={p_kruskal:.4e}\")\n",
    "            test_type = \"Kruskal-Wallis\"\n",
    "            test_stat = h_stat\n",
    "            p_val = p_kruskal\n",
    "        \n",
    "        stats_results.append({\n",
    "            'Feature': col,\n",
    "            'Test': test_type,\n",
    "            'Test_Statistic': test_stat,\n",
    "            'P_Value': p_val,\n",
    "            'Significant': p_val < ANALYSIS_CONFIG['significance_level']\n",
    "        })\n",
    "    \n",
    "    # Create summary table\n",
    "    results_df = pd.DataFrame(stats_results)\n",
    "    print(\"\\nSummary Table:\")\n",
    "    print(results_df.round(5))\n",
    "    \n",
    "    # Show significant features\n",
    "    significant = results_df[results_df['Significant']]\n",
    "    if not significant.empty:\n",
    "        print(f\"\\nFeatures with significant class differences (p < {ANALYSIS_CONFIG['significance_level']}):\")\n",
    "        print(significant[['Feature', 'Test', 'P_Value']])\n",
    "        \n",
    "        # Create boxplots for significant features\n",
    "        print(\"\\nBoxplots for features with significant class differences:\")\n",
    "        for col in significant['Feature']:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.boxplot(data=df, x=target_col, y=col)\n",
    "            plt.title(f'{col} by {target_col} (p={significant[significant[\"Feature\"]==col][\"P_Value\"].iloc[0]:.4f})')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plots_folder}/statistical_test_{col}_boxplot_{target_col}.png\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"\\nNo statistically significant feature differences found at p < {ANALYSIS_CONFIG['significance_level']}.\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    clustering_analysis(df, target_col='class')\n",
    "    statistical_tests(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e226fb9",
   "metadata": {},
   "source": [
    "## üìà Key Findings & Conclusions\n",
    "\n",
    "### üîç **Dataset Quality**\n",
    "- Clean dataset with no missing values\n",
    "- 310 patient records with 6 biomechanical features\n",
    "- Class imbalance present (especially in 3-class problem)\n",
    "\n",
    "### üìä **Feature Insights**\n",
    "- Most features show non-normal distributions\n",
    "- Outliers present but not excessive\n",
    "- Strong correlations between some biomechanical measurements\n",
    "\n",
    "### üéØ **Classification Insights**\n",
    "- Binary classification shows better class balance\n",
    "- Significant differences between groups for most features\n",
    "- Clear patterns visible in dimensionality reduction plots\n",
    "\n",
    "### üî¨ **Statistical Findings**\n",
    "- Multiple features show significant group differences (p < 0.05)\n",
    "- ANOVA/Kruskal-Wallis tests reveal discriminative power\n",
    "- Clustering analysis reveals natural groupings\n",
    "\n",
    "### üí° **Recommendations for Modeling**\n",
    "1. **Preprocessing**: Consider standardization due to different scales\n",
    "2. **Feature Selection**: High VIF features may need attention\n",
    "3. **Class Imbalance**: SMOTE or other techniques may be beneficial\n",
    "4. **Model Choice**: Non-linear models may perform better given feature distributions\n",
    "\n",
    "### üìÇ **Generated Outputs**\n",
    "- All visualizations saved in `plots/` folder\n",
    "- Detailed analysis report saved as `reports.txt`\n",
    "- Ready for machine learning pipeline implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74da5da",
   "metadata": {},
   "source": [
    "## üöÄ Running the Complete Analysis\n",
    "\n",
    "Now we'll execute all analysis functions systematically for both target variables (3-class and binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run complete analysis for different target variables\n",
    "def run_complete_analysis(df, target_variables=['class']):\n",
    "    \"\"\"\n",
    "    Run the complete EDA pipeline for specified target variables\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Running complete EDA analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    for target_col in target_variables:\n",
    "        if target_col not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Target column '{target_col}' not found in dataset\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüéØ ANALYZING TARGET: {target_col}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Run all analyses for this target\n",
    "            dataset_overview(df)\n",
    "            target_analysis(df, target_col=target_col)\n",
    "            numerical_features_analysis(df, target_col=target_col)\n",
    "            outlier_analysis(df, target_col=target_col)\n",
    "            correlation_analysis(df, target_col=target_col)\n",
    "            feature_relationships(df, target_col=target_col)\n",
    "            dimensionality_analysis(df, target_col=target_col)\n",
    "            clustering_analysis(df, target_col=target_col)\n",
    "            statistical_tests(df, target_col=target_col)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Analysis complete for {target_col}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing {target_col}: {e}\")\n",
    "    \n",
    "    print(\"\\nüéâ COMPLETE EDA ANALYSIS FINISHED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÅ All plots saved to: {plots_folder}\")\n",
    "    print(\"üìä Plots displayed inline in notebook cells\")\n",
    "    print(\"üîç Review the plots folder for saved visualizations.\")\n",
    "\n",
    "# Uncomment the line below to run analysis for multiple targets\n",
    "# run_complete_analysis(df, target_variables=['class', 'binary_class'])\n",
    "\n",
    "print(\"\\nüîß Optional batch execution function defined!\")\n",
    "print(\"üìù Individual cells above run immediately when executed.\")\n",
    "print(\"‚ö° Use run_complete_analysis() for batch processing multiple targets.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
