{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5404d83",
   "metadata": {},
   "source": [
    "# üè• Comprehensive Exploratory Data Analysis\n",
    "## Biomechanical Features of Orthopedic Patients\n",
    "\n",
    "### üìã Executive Summary\n",
    "This notebook provides a comprehensive exploratory data analysis of biomechanical features in orthopedic patients. The analysis includes statistical tests, advanced visualizations, dimensionality reduction, and clustering techniques to understand the relationship between biomechanical measurements and orthopedic conditions.\n",
    "\n",
    "### üéØ Key Objectives\n",
    "1. **Dataset Overview**: Understand data structure, missing values, and basic statistics\n",
    "2. **Target Analysis**: Examine class distribution and balance\n",
    "3. **Feature Analysis**: Analyze numerical features, distributions, and outliers\n",
    "4. **Correlation Analysis**: Identify multicollinearity and feature relationships\n",
    "5. **Statistical Testing**: Perform hypothesis testing for group differences\n",
    "6. **Dimensionality Reduction**: Apply PCA and t-SNE for visualization\n",
    "7. **Clustering**: Discover natural groupings in the data\n",
    "\n",
    "### üìä Dataset Information\n",
    "- **Source**: UCI Machine Learning Repository\n",
    "- **Features**: 6 biomechanical attributes\n",
    "- **Target**: 3-class (Normal, Hernia, Spondylolisthesis) and binary classification\n",
    "- **Size**: 310 patient records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6f702c",
   "metadata": {},
   "source": [
    "### Comprehensive Exploratory Data Analysis\n",
    "##### Biomechanical Features of Orthopedic Patients\n",
    "This notebook provides a comprehensive exploratory data analysis of biomechanical features in orthopedic patients, including statistical tests, visualizations, and dimensionality reduction techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81880f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'random_state': 42,\n",
    "    'test_size': 0.2,\n",
    "    'significance_level': 0.05,\n",
    "    'correlation_threshold': 0.8,\n",
    "    'pca_n_components': 2,\n",
    "    'tsne_n_components': 2,\n",
    "    'tsne_perplexity': 30,\n",
    "    'umap_n_components': 2,\n",
    "    'umap_n_neighbors': 15,\n",
    "    'figure_size': (12, 8),\n",
    "    'plot_dpi': 300,\n",
    "    'save_plots': True,\n",
    "    'plots_folder': 'plots'\n",
    "}\n",
    "\n",
    "print(\"Configuration parameters loaded successfully!\")\n",
    "print(\"Available parameters:\")\n",
    "for key, value in ANALYSIS_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f5682a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom output handler for better visualization\n",
    "class MultiOut:\n",
    "    def __init__(self, *outputs):\n",
    "        self.outputs = outputs\n",
    "    \n",
    "    def write(self, s):\n",
    "        for out in self.outputs:\n",
    "            out.write(s)\n",
    "    \n",
    "    def flush(self):\n",
    "        for out in self.outputs:\n",
    "            out.flush()\n",
    "\n",
    "print(\"MultiOut class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd6e0a",
   "metadata": {},
   "source": [
    "### üéõÔ∏è Analysis Configuration\n",
    "You can modify these parameters to customize the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa7b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis Configuration Parameters\n",
    "ANALYSIS_CONFIG = {\n",
    "    'correlation_threshold': 0.7,  # Threshold for high correlation\n",
    "    'outlier_z_threshold': 3.0,    # Z-score threshold for outliers\n",
    "    'significance_level': 0.05,    # P-value threshold for statistical tests\n",
    "    'pca_variance_threshold': 0.95, # Variance threshold for PCA\n",
    "    'figure_size': (15, 10),       # Default figure size\n",
    "    'save_plots': True,            # Whether to save plots\n",
    "    'plot_format': 'png',          # Plot save format\n",
    "    'random_state': 42             # Random state for reproducibility\n",
    "}\n",
    "\n",
    "print(\"üìã Analysis Configuration:\")\n",
    "for key, value in ANALYSIS_CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fba234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import levene\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import kagglehub\n",
    "import umap\n",
    "\n",
    "# Set up plotting configuration for inline display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(\"Available packages:\")\n",
    "print(\"- pandas, numpy, matplotlib, seaborn\")\n",
    "print(\"- scipy, sklearn, umap\")\n",
    "print(\"- kagglehub for data access\")\n",
    "print(\"- warnings and os for system operations\")\n",
    "print(\"üìä Matplotlib configured for inline display + file saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c511d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots directory and load data\n",
    "plots_folder = ANALYSIS_CONFIG['plots_folder']\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)\n",
    "    print(f\"Created plots directory: {plots_folder}\")\n",
    "else:\n",
    "    print(f\"Plots directory already exists: {plots_folder}\")\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load the Vertebral Column dataset\"\"\"\n",
    "    try:\n",
    "        # Download latest version\n",
    "        path = kagglehub.dataset_download(\"sammy123/lower-back-pain-symptoms-dataset\")\n",
    "        print(f\"Dataset downloaded to: {path}\")\n",
    "        \n",
    "        # Load the dataset\n",
    "        data_path = os.path.join(path, \"Dataset_spine.csv\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load the data immediately\n",
    "print(\"üöÄ Loading dataset...\")\n",
    "df = load_data()\n",
    "\n",
    "if df is not None:\n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to load dataset. Please check the data source.\")\n",
    "    \n",
    "print(\"\\nüîß Data loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ead59",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddbaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_overview(df):\n",
    "    \"\"\"\n",
    "    Provide comprehensive overview of the dataset including:\n",
    "    - Basic information (shape, columns, data types)\n",
    "    - Missing values analysis\n",
    "    - Basic statistics\n",
    "    - Data quality assessment\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"1. DATASET OVERVIEW\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Features: {df.shape[1]}\")\n",
    "    print(f\"Samples: {df.shape[0]}\")\n",
    "    \n",
    "    print(\"\\nColumn Information:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nData Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(missing_values[missing_values > 0])\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nLast 5 rows:\")\n",
    "    print(df.tail())\n",
    "    \n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    dataset_overview(df)\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2852700f",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb983e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze the target variable including:\n",
    "    - Value counts and proportions\n",
    "    - Visualization of class distribution\n",
    "    - Class balance assessment\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"2. TARGET VARIABLE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Value counts and proportions\n",
    "    value_counts = df[target_col].value_counts()\n",
    "    proportions = df[target_col].value_counts(normalize=True)\n",
    "    \n",
    "    print(f\"Target variable: {target_col}\")\n",
    "    print(f\"Unique classes: {df[target_col].nunique()}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    \n",
    "    for class_name, count in value_counts.items():\n",
    "        percentage = proportions[class_name] * 100\n",
    "        print(f\"  {class_name}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Count plot\n",
    "    sns.countplot(data=df, x=target_col, ax=axes[0])\n",
    "    axes[0].set_title('Class Distribution (Count)')\n",
    "    axes[0].set_xlabel('Class')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    \n",
    "    # Add count labels on bars\n",
    "    for i, v in enumerate(value_counts.values):\n",
    "        axes[0].text(i, v + 0.5, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # Pie chart\n",
    "    axes[1].pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[1].set_title('Class Distribution (Proportion)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/target_distribution_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Class balance assessment\n",
    "    max_class = value_counts.max()\n",
    "    min_class = value_counts.min()\n",
    "    balance_ratio = min_class / max_class\n",
    "    \n",
    "    print(f\"\\nClass Balance Assessment:\")\n",
    "    print(f\"Most frequent class: {value_counts.idxmax()} ({max_class} samples)\")\n",
    "    print(f\"Least frequent class: {value_counts.idxmin()} ({min_class} samples)\")\n",
    "    print(f\"Balance ratio: {balance_ratio:.3f}\")\n",
    "    \n",
    "    if balance_ratio < 0.5:\n",
    "        print(\"‚ö†Ô∏è  Dataset is imbalanced - consider using stratified sampling or class weighting\")\n",
    "    else:\n",
    "        print(\"‚úÖ Dataset is relatively balanced\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    target_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45930",
   "metadata": {},
   "source": [
    "## 4. Numerical Features Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_features_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze numerical features including:\n",
    "    - Distribution plots\n",
    "    - Box plots by class\n",
    "    - Statistical summaries by class\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"3. NUMERICAL FEATURES ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    print(f\"Numerical features: {len(numerical_cols)}\")\n",
    "    print(f\"Features: {numerical_cols}\")\n",
    "    \n",
    "    # Distribution plots\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        axes[row, col_idx].hist(df[col], bins=30, alpha=0.7, edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/numerical_features_distribution_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Box plots by class\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        sns.boxplot(data=df, x=target_col, y=col, ax=axes[row, col_idx])\n",
    "        axes[row, col_idx].set_title(f'{col} by {target_col}')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/numerical_features_boxplots_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical summaries by class\n",
    "    print(\"\\nStatistical Summary by Class:\")\n",
    "    for class_name in df[target_col].unique():\n",
    "        print(f\"\\n{class_name}:\")\n",
    "        print(df[df[target_col] == class_name][numerical_cols].describe())\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    numerical_features_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7f4b3",
   "metadata": {},
   "source": [
    "## 5. Outlier Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b762fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outlier_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Detect and analyze outliers using:\n",
    "    - IQR method\n",
    "    - Z-score method\n",
    "    - Isolation Forest\n",
    "    - Visualization of outliers\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"4. OUTLIER DETECTION AND ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    outlier_summary = {}\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\nAnalyzing outliers in {col}:\")\n",
    "        \n",
    "        # IQR method\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        iqr_outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "        \n",
    "        # Z-score method\n",
    "        z_scores = np.abs(stats.zscore(df[col]))\n",
    "        z_outliers = df[z_scores > 3]\n",
    "        \n",
    "        print(f\"  IQR method: {len(iqr_outliers)} outliers ({len(iqr_outliers)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  Z-score method: {len(z_outliers)} outliers ({len(z_outliers)/len(df)*100:.1f}%)\")\n",
    "        print(f\"  IQR bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "        \n",
    "        outlier_summary[col] = {\n",
    "            'IQR_outliers': len(iqr_outliers),\n",
    "            'Z_outliers': len(z_outliers),\n",
    "            'IQR_percentage': len(iqr_outliers)/len(df)*100,\n",
    "            'Z_percentage': len(z_outliers)/len(df)*100\n",
    "        }\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Box plot for all features\n",
    "    df_scaled = df[numerical_cols].copy()\n",
    "    for col in numerical_cols:\n",
    "        df_scaled[col] = (df_scaled[col] - df_scaled[col].mean()) / df_scaled[col].std()\n",
    "    \n",
    "    axes[0, 0].boxplot([df_scaled[col] for col in numerical_cols])\n",
    "    axes[0, 0].set_xticklabels(numerical_cols, rotation=45)\n",
    "    axes[0, 0].set_title('Box Plot (Standardized Features)')\n",
    "    axes[0, 0].set_ylabel('Standardized Values')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Outlier count by method\n",
    "    methods = ['IQR', 'Z-score']\n",
    "    iqr_counts = [outlier_summary[col]['IQR_outliers'] for col in numerical_cols]\n",
    "    z_counts = [outlier_summary[col]['Z_outliers'] for col in numerical_cols]\n",
    "    \n",
    "    x = np.arange(len(numerical_cols))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0, 1].bar(x - width/2, iqr_counts, width, label='IQR', alpha=0.7)\n",
    "    axes[0, 1].bar(x + width/2, z_counts, width, label='Z-score', alpha=0.7)\n",
    "    axes[0, 1].set_xlabel('Features')\n",
    "    axes[0, 1].set_ylabel('Number of Outliers')\n",
    "    axes[0, 1].set_title('Outlier Count by Method')\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(numerical_cols, rotation=45)\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Outlier percentage\n",
    "    iqr_percentages = [outlier_summary[col]['IQR_percentage'] for col in numerical_cols]\n",
    "    z_percentages = [outlier_summary[col]['Z_percentage'] for col in numerical_cols]\n",
    "    \n",
    "    axes[1, 0].bar(x - width/2, iqr_percentages, width, label='IQR', alpha=0.7)\n",
    "    axes[1, 0].bar(x + width/2, z_percentages, width, label='Z-score', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Features')\n",
    "    axes[1, 0].set_ylabel('Percentage of Outliers')\n",
    "    axes[1, 0].set_title('Outlier Percentage by Method')\n",
    "    axes[1, 0].set_xticks(x)\n",
    "    axes[1, 0].set_xticklabels(numerical_cols, rotation=45)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary heatmap\n",
    "    summary_df = pd.DataFrame(outlier_summary).T\n",
    "    sns.heatmap(summary_df[['IQR_percentage', 'Z_percentage']], \n",
    "                annot=True, fmt='.1f', cmap='Reds', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Outlier Percentage Heatmap')\n",
    "    axes[1, 1].set_ylabel('Features')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/outlier_detection_summary.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Overall summary\n",
    "    print(\"\\nOutlier Summary:\")\n",
    "    summary_df = pd.DataFrame(outlier_summary).T\n",
    "    print(summary_df.round(2))\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    outlier_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a4ed50",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15023eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze correlations between features:\n",
    "    - Correlation matrix\n",
    "    - Heatmap visualization\n",
    "    - Identify highly correlated features\n",
    "    - Multicollinearity assessment\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"5. CORRELATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    corr_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    print(f\"Correlation matrix shape: {corr_matrix.shape}\")\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Standard correlation heatmap\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "                cmap='RdBu_r', center=0, ax=axes[0, 0])\n",
    "    axes[0, 0].set_title('Correlation Matrix (Lower Triangle)')\n",
    "    \n",
    "    # Clustermap for grouping similar correlations\n",
    "    corr_linkage = sns.clustermap(corr_matrix, method='ward', cmap='RdBu_r', \n",
    "                                 center=0, figsize=(10, 8), \n",
    "                                 cbar_pos=(0.02, 0.8, 0.05, 0.15))\n",
    "    axes[0, 1].set_title('Correlation Clustermap')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    # High correlation pairs\n",
    "    threshold = ANALYSIS_CONFIG['correlation_threshold']\n",
    "    high_corr_pairs = []\n",
    "    \n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            corr_val = corr_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > threshold:\n",
    "                high_corr_pairs.append({\n",
    "                    'Feature1': corr_matrix.columns[i],\n",
    "                    'Feature2': corr_matrix.columns[j],\n",
    "                    'Correlation': corr_val\n",
    "                })\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(f\"\\nHighly correlated feature pairs (|r| > {threshold}):\")\n",
    "        for pair in high_corr_pairs:\n",
    "            print(f\"  {pair['Feature1']} - {pair['Feature2']}: {pair['Correlation']:.3f}\")\n",
    "        \n",
    "        # Visualize high correlation pairs\n",
    "        high_corr_df = pd.DataFrame(high_corr_pairs)\n",
    "        high_corr_df['abs_correlation'] = high_corr_df['Correlation'].abs()\n",
    "        \n",
    "        bars = axes[1, 0].bar(range(len(high_corr_df)), high_corr_df['abs_correlation'])\n",
    "        axes[1, 0].set_xlabel('Feature Pairs')\n",
    "        axes[1, 0].set_ylabel('|Correlation|')\n",
    "        axes[1, 0].set_title(f'High Correlation Pairs (|r| > {threshold})')\n",
    "        axes[1, 0].set_xticks(range(len(high_corr_df)))\n",
    "        axes[1, 0].set_xticklabels([f\"{pair['Feature1']}-{pair['Feature2']}\" for pair in high_corr_pairs], \n",
    "                                  rotation=45, ha='right')\n",
    "        axes[1, 0].axhline(y=threshold, color='red', linestyle='--', alpha=0.7)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        print(f\"\\nNo highly correlated feature pairs found (|r| > {threshold})\")\n",
    "        axes[1, 0].text(0.5, 0.5, f'No high correlations\\n(|r| > {threshold})', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('High Correlation Analysis')\n",
    "    \n",
    "    # Multicollinearity assessment using VIF\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    \n",
    "    try:\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Feature\"] = numerical_cols\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(df[numerical_cols].values, i) \n",
    "                          for i in range(len(numerical_cols))]\n",
    "        \n",
    "        # Plot VIF values\n",
    "        bars = axes[1, 1].bar(range(len(vif_data)), vif_data['VIF'])\n",
    "        axes[1, 1].set_xlabel('Features')\n",
    "        axes[1, 1].set_ylabel('VIF Value')\n",
    "        axes[1, 1].set_title('Variance Inflation Factor (VIF)')\n",
    "        axes[1, 1].set_xticks(range(len(vif_data)))\n",
    "        axes[1, 1].set_xticklabels(vif_data['Feature'], rotation=45, ha='right')\n",
    "        axes[1, 1].axhline(y=5, color='orange', linestyle='--', alpha=0.7, label='Moderate (5)')\n",
    "        axes[1, 1].axhline(y=10, color='red', linestyle='--', alpha=0.7, label='High (10)')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        print(\"\\nVariance Inflation Factor (VIF) Analysis:\")\n",
    "        print(vif_data.round(2))\n",
    "        \n",
    "        high_vif = vif_data[vif_data['VIF'] > 5]\n",
    "        if not high_vif.empty:\n",
    "            print(f\"\\nFeatures with high VIF (>5):\")\n",
    "            print(high_vif)\n",
    "        else:\n",
    "            print(\"\\nNo features with high multicollinearity detected (VIF > 5)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating VIF: {e}\")\n",
    "        axes[1, 1].text(0.5, 0.5, 'VIF calculation\\nfailed', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/correlation_analysis.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Save clustermap separately\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.clustermap(corr_matrix, method='ward', cmap='RdBu_r', center=0, \n",
    "                  cbar_pos=(0.02, 0.8, 0.05, 0.15))\n",
    "    plt.savefig(f\"{plots_folder}/correlation_clustermap.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    correlation_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c59664",
   "metadata": {},
   "source": [
    "## 7. Feature Relationships Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daf4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_relationships(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Analyze relationships between features:\n",
    "    - Pairplot\n",
    "    - Feature interactions\n",
    "    - Violin plots by class\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"6. FEATURE RELATIONSHIPS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Pairplot (sample if too many features)\n",
    "    if len(numerical_cols) > 6:\n",
    "        print(f\"Too many features ({len(numerical_cols)}) for pairplot. Showing first 6.\")\n",
    "        cols_for_pairplot = numerical_cols[:6]\n",
    "    else:\n",
    "        cols_for_pairplot = numerical_cols\n",
    "    \n",
    "    print(f\"Creating pairplot for features: {cols_for_pairplot}\")\n",
    "    \n",
    "    # Create pairplot\n",
    "    pairplot_df = df[cols_for_pairplot + [target_col]]\n",
    "    g = sns.pairplot(pairplot_df, hue=target_col, diag_kind='kde', \n",
    "                     plot_kws={'alpha': 0.6}, diag_kws={'alpha': 0.7})\n",
    "    g.fig.suptitle('Feature Pairplot by Class', y=1.02)\n",
    "    \n",
    "    plt.savefig(f\"{plots_folder}/feature_pairplot.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature interactions - violin plots\n",
    "    n_cols = 3\n",
    "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        \n",
    "        sns.violinplot(data=df, x=target_col, y=col, ax=axes[row, col_idx])\n",
    "        axes[row, col_idx].set_title(f'{col} Distribution by {target_col}')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for i in range(len(numerical_cols), n_rows * n_cols):\n",
    "        row = i // n_cols\n",
    "        col_idx = i % n_cols\n",
    "        axes[row, col_idx].remove()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/feature_interactions_violin_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate feature importance using correlation with target\n",
    "    if target_col in df.columns:\n",
    "        # Create numerical encoding of target for correlation\n",
    "        target_encoded = pd.get_dummies(df[target_col], drop_first=True)\n",
    "        \n",
    "        if target_encoded.shape[1] == 1:  # Binary classification\n",
    "            target_numeric = target_encoded.iloc[:, 0]\n",
    "            correlations = df[numerical_cols].corrwith(target_numeric)\n",
    "            \n",
    "            print(\"\\nFeature-Target Correlations:\")\n",
    "            correlations_sorted = correlations.abs().sort_values(ascending=False)\n",
    "            print(correlations_sorted.round(3))\n",
    "            \n",
    "            # Plot feature importance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            correlations_sorted.plot(kind='bar')\n",
    "            plt.title('Feature Importance (Absolute Correlation with Target)')\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('|Correlation|')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plots_folder}/feature_importance_correlation.png\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"Multi-class target detected - correlation analysis skipped\")\n",
    "    \n",
    "    print(f\"\\nFeature relationship analysis completed for {len(numerical_cols)} features\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    feature_relationships(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fb6f84",
   "metadata": {},
   "source": [
    "## 8. Dimensionality Reduction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eba347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensionality_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction analysis:\n",
    "    - PCA analysis with explained variance\n",
    "    - t-SNE visualization\n",
    "    - UMAP visualization\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"7. DIMENSIONALITY REDUCTION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numerical_cols].dropna()\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # PCA Analysis\n",
    "    pca = PCA()\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"Original dimensions: {X_scaled.shape}\")\n",
    "    print(f\"PCA components: {pca.n_components_}\")\n",
    "    \n",
    "    # Explained variance\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "    \n",
    "    print(f\"First 2 components explain {cumulative_variance[1]:.3f} of variance\")\n",
    "    print(f\"First 3 components explain {cumulative_variance[2]:.3f} of variance\")\n",
    "    \n",
    "    # Find number of components for 95% variance\n",
    "    n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "    print(f\"Components needed for 95% variance: {n_components_95}\")\n",
    "    \n",
    "    # t-SNE\n",
    "    print(\"\\nComputing t-SNE...\")\n",
    "    tsne = TSNE(n_components=ANALYSIS_CONFIG['tsne_n_components'], \n",
    "                perplexity=ANALYSIS_CONFIG['tsne_perplexity'],\n",
    "                random_state=ANALYSIS_CONFIG['random_state'])\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    # UMAP\n",
    "    print(\"Computing UMAP...\")\n",
    "    reducer = umap.UMAP(n_components=ANALYSIS_CONFIG['umap_n_components'],\n",
    "                       n_neighbors=ANALYSIS_CONFIG['umap_n_neighbors'],\n",
    "                       random_state=ANALYSIS_CONFIG['random_state'])\n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # PCA explained variance\n",
    "    axes[0, 0].bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "    axes[0, 0].set_xlabel('Principal Component')\n",
    "    axes[0, 0].set_ylabel('Explained Variance Ratio')\n",
    "    axes[0, 0].set_title('PCA Explained Variance')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cumulative explained variance\n",
    "    axes[0, 1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "    axes[0, 1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95%')\n",
    "    axes[0, 1].set_xlabel('Number of Components')\n",
    "    axes[0, 1].set_ylabel('Cumulative Explained Variance')\n",
    "    axes[0, 1].set_title('Cumulative Explained Variance')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # PCA visualization\n",
    "    scatter = axes[0, 2].scatter(X_pca[:, 0], X_pca[:, 1], c=y.astype('category').cat.codes, \n",
    "                                cmap='viridis', alpha=0.6)\n",
    "    axes[0, 2].set_xlabel(f'PC1 ({explained_variance_ratio[0]:.3f})')\n",
    "    axes[0, 2].set_ylabel(f'PC2 ({explained_variance_ratio[1]:.3f})')\n",
    "    axes[0, 2].set_title('PCA Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[0, 2])\n",
    "    \n",
    "    # t-SNE visualization\n",
    "    scatter = axes[1, 0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y.astype('category').cat.codes, \n",
    "                                cmap='viridis', alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('t-SNE 1')\n",
    "    axes[1, 0].set_ylabel('t-SNE 2')\n",
    "    axes[1, 0].set_title('t-SNE Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # UMAP visualization\n",
    "    scatter = axes[1, 1].scatter(X_umap[:, 0], X_umap[:, 1], c=y.astype('category').cat.codes, \n",
    "                                cmap='viridis', alpha=0.6)\n",
    "    axes[1, 1].set_xlabel('UMAP 1')\n",
    "    axes[1, 1].set_ylabel('UMAP 2')\n",
    "    axes[1, 1].set_title('UMAP Visualization')\n",
    "    plt.colorbar(scatter, ax=axes[1, 1])\n",
    "    \n",
    "    # Feature loadings for first two PCs\n",
    "    loadings = pca.components_[:2].T\n",
    "    axes[1, 2].scatter(loadings[:, 0], loadings[:, 1], alpha=0.7)\n",
    "    for i, feature in enumerate(numerical_cols):\n",
    "        axes[1, 2].annotate(feature, (loadings[i, 0], loadings[i, 1]), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[1, 2].set_xlabel('PC1 Loading')\n",
    "    axes[1, 2].set_ylabel('PC2 Loading')\n",
    "    axes[1, 2].set_title('Feature Loadings (PC1 vs PC2)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/dimensionality_reduction_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature importance from PCA\n",
    "    print(\"\\nFeature importance from first 2 PCs:\")\n",
    "    feature_importance = np.abs(loadings).mean(axis=1)\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': numerical_cols,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(feature_importance_df.round(3))\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    dimensionality_analysis(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f71b3d3",
   "metadata": {},
   "source": [
    "## 9. Clustering Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b095a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_analysis(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Perform clustering analysis:\n",
    "    - K-means clustering with optimal k selection\n",
    "    - Elbow method for k selection\n",
    "    - Silhouette analysis\n",
    "    - Comparison with true labels\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"8. CLUSTERING ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    X = df[numerical_cols].dropna()\n",
    "    \n",
    "    # Standardize features for clustering\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Elbow method: calculate inertia for different k values\n",
    "    inertias = []\n",
    "    k_range = range(1, 11)\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=ANALYSIS_CONFIG['random_state'], n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Silhouette analysis for optimal k\n",
    "    silhouette_scores = []\n",
    "    for k in range(2, 11):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=ANALYSIS_CONFIG['random_state'], n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "        silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "    \n",
    "    # Find optimal k based on silhouette score\n",
    "    optimal_k = range(2, 11)[np.argmax(silhouette_scores)]\n",
    "    print(f\"Optimal number of clusters (silhouette): {optimal_k}\")\n",
    "    print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "    # Perform clustering with optimal k\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=ANALYSIS_CONFIG['random_state'], n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "    \n",
    "    # Add cluster labels to dataframe\n",
    "    df_cluster = df.copy()\n",
    "    df_cluster['Cluster'] = cluster_labels\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Elbow method plot\n",
    "    axes[0, 0].plot(k_range, inertias, 'bo-')\n",
    "    axes[0, 0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 0].set_ylabel('Inertia')\n",
    "    axes[0, 0].set_title('Elbow Method for Optimal k')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette score plot\n",
    "    axes[0, 1].plot(range(2, 11), silhouette_scores, 'ro-')\n",
    "    axes[0, 1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0, 1].set_ylabel('Silhouette Score')\n",
    "    axes[0, 1].set_title('Silhouette Analysis')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Clusters in PCA space\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "    scatter = axes[1, 0].scatter(pca_result[:, 0], pca_result[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    axes[1, 0].set_xlabel('First Principal Component')\n",
    "    axes[1, 0].set_ylabel('Second Principal Component')\n",
    "    axes[1, 0].set_title('Clusters in PCA Space')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # Clusters vs true labels\n",
    "    cluster_crosstab = pd.crosstab(df_cluster['Cluster'], df_cluster[target_col])\n",
    "    sns.heatmap(cluster_crosstab, annot=True, fmt='d', cmap='Blues', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Clusters vs True Labels')\n",
    "    axes[1, 1].set_xlabel('True Labels')\n",
    "    axes[1, 1].set_ylabel('Clusters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{plots_folder}/clustering_analysis_{target_col}.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nCluster vs True Labels Cross-tabulation:\")\n",
    "    print(cluster_crosstab)\n",
    "\n",
    "\n",
    "def statistical_tests(df, target_col='class'):\n",
    "    \"\"\"\n",
    "    Perform statistical tests to identify significant differences between groups\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"9. STATISTICAL TESTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    groups = df.groupby(target_col)\n",
    "    group_names = list(groups.groups.keys())\n",
    "    stats_results = []\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        print(f\"\\nFeature: {col}\")\n",
    "        print(\"-\" * 20)\n",
    "        feat_group_data = [df[df[target_col] == name][col].dropna() for name in group_names]\n",
    "        \n",
    "        # Test for normality\n",
    "        normality = []\n",
    "        for i, data in enumerate(feat_group_data):\n",
    "            if len(data) > 3:\n",
    "                _, pval = stats.shapiro(data)\n",
    "                normality.append(pval > 0.05)\n",
    "                print(f\"  {group_names[i]}: Shapiro p={pval:.4f} ({'Normal' if pval>0.05 else 'Non-normal'})\")\n",
    "            else:\n",
    "                normality.append(False)\n",
    "                print(f\"  {group_names[i]}: Not enough data for normality test\")\n",
    "        \n",
    "        # Test for equal variances\n",
    "        if all([len(x)>3 for x in feat_group_data]):\n",
    "            _, p_levene = levene(*feat_group_data)\n",
    "            print(f\"  Levene's p={p_levene:.4f} ({'Equal variances' if p_levene>0.05 else 'Unequal variances'})\")\n",
    "        else:\n",
    "            p_levene = np.nan\n",
    "        \n",
    "        # Choose appropriate test\n",
    "        if all(normality) and (p_levene > 0.05):\n",
    "            # Use ANOVA if data is normal and variances are equal\n",
    "            f_stat, p_anova = stats.f_oneway(*feat_group_data)\n",
    "            print(f\"  ANOVA F={f_stat:.3f}, p={p_anova:.4e}\")\n",
    "            test_type = \"ANOVA\"\n",
    "            test_stat = f_stat\n",
    "            p_val = p_anova\n",
    "        else:\n",
    "            # Use Kruskal-Wallis if assumptions are violated\n",
    "            h_stat, p_kruskal = stats.kruskal(*feat_group_data)\n",
    "            print(f\"  Kruskal-Wallis H={h_stat:.3f}, p={p_kruskal:.4e}\")\n",
    "            test_type = \"Kruskal-Wallis\"\n",
    "            test_stat = h_stat\n",
    "            p_val = p_kruskal\n",
    "        \n",
    "        stats_results.append({\n",
    "            'Feature': col,\n",
    "            'Test': test_type,\n",
    "            'Test_Statistic': test_stat,\n",
    "            'P_Value': p_val,\n",
    "            'Significant': p_val < ANALYSIS_CONFIG['significance_level']\n",
    "        })\n",
    "    \n",
    "    # Create summary table\n",
    "    results_df = pd.DataFrame(stats_results)\n",
    "    print(\"\\nSummary Table:\")\n",
    "    print(results_df.round(5))\n",
    "    \n",
    "    # Show significant features\n",
    "    significant = results_df[results_df['Significant']]\n",
    "    if not significant.empty:\n",
    "        print(f\"\\nFeatures with significant class differences (p < {ANALYSIS_CONFIG['significance_level']}):\")\n",
    "        print(significant[['Feature', 'Test', 'P_Value']])\n",
    "        \n",
    "        # Create boxplots for significant features\n",
    "        print(\"\\nBoxplots for features with significant class differences:\")\n",
    "        for col in significant['Feature']:\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.boxplot(data=df, x=target_col, y=col)\n",
    "            plt.title(f'{col} by {target_col} (p={significant[significant[\"Feature\"]==col][\"P_Value\"].iloc[0]:.4f})')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{plots_folder}/statistical_test_{col}_boxplot_{target_col}.png\", bbox_inches=\"tight\")\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"\\nNo statistically significant feature differences found at p < {ANALYSIS_CONFIG['significance_level']}.\")\n",
    "\n",
    "# Execute immediately\n",
    "if df is not None:\n",
    "    clustering_analysis(df, target_col='class')\n",
    "    statistical_tests(df, target_col='class')\n",
    "else:\n",
    "    print(\"‚ùå No data available for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e226fb9",
   "metadata": {},
   "source": [
    "## üìà Key Findings & Conclusions\n",
    "\n",
    "### üîç **Dataset Quality**\n",
    "- Clean dataset with no missing values\n",
    "- 310 patient records with 6 biomechanical features\n",
    "- Class imbalance present (especially in 3-class problem)\n",
    "\n",
    "### üìä **Feature Insights**\n",
    "- Most features show non-normal distributions\n",
    "- Outliers present but not excessive\n",
    "- Strong correlations between some biomechanical measurements\n",
    "\n",
    "### üéØ **Classification Insights**\n",
    "- Binary classification shows better class balance\n",
    "- Significant differences between groups for most features\n",
    "- Clear patterns visible in dimensionality reduction plots\n",
    "\n",
    "### üî¨ **Statistical Findings**\n",
    "- Multiple features show significant group differences (p < 0.05)\n",
    "- ANOVA/Kruskal-Wallis tests reveal discriminative power\n",
    "- Clustering analysis reveals natural groupings\n",
    "\n",
    "### üí° **Recommendations for Modeling**\n",
    "1. **Preprocessing**: Consider standardization due to different scales\n",
    "2. **Feature Selection**: High VIF features may need attention\n",
    "3. **Class Imbalance**: SMOTE or other techniques may be beneficial\n",
    "4. **Model Choice**: Non-linear models may perform better given feature distributions\n",
    "\n",
    "### üìÇ **Generated Outputs**\n",
    "- All visualizations saved in `plots/` folder\n",
    "- Detailed analysis report saved as `reports.txt`\n",
    "- Ready for machine learning pipeline implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74da5da",
   "metadata": {},
   "source": [
    "## üöÄ Running the Complete Analysis\n",
    "\n",
    "Now we'll execute all analysis functions systematically for both target variables (3-class and binary classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e6f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run complete analysis for different target variables\n",
    "def run_complete_analysis(df, target_variables=['class']):\n",
    "    \"\"\"\n",
    "    Run the complete EDA pipeline for specified target variables\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Running complete EDA analysis...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    if df is None:\n",
    "        print(\"‚ùå No data available for analysis\")\n",
    "        return\n",
    "    \n",
    "    for target_col in target_variables:\n",
    "        if target_col not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Target column '{target_col}' not found in dataset\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüéØ ANALYZING TARGET: {target_col}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        try:\n",
    "            # Run all analyses for this target\n",
    "            dataset_overview(df)\n",
    "            target_analysis(df, target_col=target_col)\n",
    "            numerical_features_analysis(df, target_col=target_col)\n",
    "            outlier_analysis(df, target_col=target_col)\n",
    "            correlation_analysis(df, target_col=target_col)\n",
    "            feature_relationships(df, target_col=target_col)\n",
    "            dimensionality_analysis(df, target_col=target_col)\n",
    "            clustering_analysis(df, target_col=target_col)\n",
    "            statistical_tests(df, target_col=target_col)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Analysis complete for {target_col}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing {target_col}: {e}\")\n",
    "    \n",
    "    print(\"\\nüéâ COMPLETE EDA ANALYSIS FINISHED!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìÅ All plots saved to: {plots_folder}\")\n",
    "    print(\"üìä Plots displayed inline in notebook cells\")\n",
    "    print(\"üîç Review the plots folder for saved visualizations.\")\n",
    "\n",
    "# Uncomment the line below to run analysis for multiple targets\n",
    "# run_complete_analysis(df, target_variables=['class', 'binary_class'])\n",
    "\n",
    "print(\"\\nüîß Optional batch execution function defined!\")\n",
    "print(\"üìù Individual cells above run immediately when executed.\")\n",
    "print(\"‚ö° Use run_complete_analysis() for batch processing multiple targets.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
